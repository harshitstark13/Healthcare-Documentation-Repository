{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ht8F4lAIHEUY"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE64GrUrHEUa"
      },
      "source": [
        "` Data cleaning is a time consuming and unenjoyable task, yet it's a very important one. Keep in mind, \"garbage in, garbage out\".`\n",
        "\n",
        "#### Feeding dirty data into a model will give us results that are meaningless.\n",
        "\n",
        "### Objective:\n",
        "\n",
        "1. Getting the data\n",
        "2. Cleaning the data\n",
        "3. Organizing the data - organize the cleaned data into a way that is easy to input into other algorithms\n",
        "\n",
        "### Output :\n",
        "#### cleaned and organized data in two standard text formats:\n",
        "\n",
        "1. Corpus - a collection of text\n",
        "2. Document-Term Matrix - word counts in matrix format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rehb6SQoHEUb"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QXYT4ErHEUb"
      },
      "source": [
        "Look at transcripts of various comedians and note their similarities and differences and find if the stand up comedian of your choice has comedy style different than other comedian.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-giNgCpxHEUb"
      },
      "source": [
        "## Getting The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vA84O4OHEUb"
      },
      "source": [
        "You can get the transcripts of some comedian from [Scraps From The Loft](http://scrapsfromtheloft.com).\n",
        "\n",
        "You can take help of IMDB and select only 10 or 20 comedian having highest rating.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### For example:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries, Resources and Loading Data"
      ],
      "metadata": {
        "id": "XXFl_JJqN2hg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avFXjlPoHEUb",
        "outputId": "e4c66d73-0d78-4914-9219-df44bf7eba0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv('/content/mtsamples.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning The Data\n",
        "When dealing with numerical data, data cleaning often involves removing null values and duplicate data, dealing with outliers, etc. With text data, there are some common data cleaning techniques, which are also known as text pre-processing techniques.\n",
        "\n",
        "With text data, this cleaning process can go on forever. There's always an exception to every cleaning step. So, we're going to follow the MVP (minimum viable product) approach - start simple and iterate."
      ],
      "metadata": {
        "id": "-BTrBz1qn6cm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Missing Values"
      ],
      "metadata": {
        "id": "tkP8viOyN-DV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "80ewY2SCHEUc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your DataFrame is named df\n",
        "# Replace the column names as necessary\n",
        "\n",
        "# Remove rows with zero-length strings or null values in any of the specified columns\n",
        "columns_to_check = [\"description\", \"medical_specialty\", \"sample_name\", \"transcription\", \"keywords\"]\n",
        "for column in columns_to_check:\n",
        "    df = df[df[column].apply(lambda x: len(str(x).strip()) > 0 if pd.notnull(x) else False)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standardizing Text Data"
      ],
      "metadata": {
        "id": "WmgWaD6_Oh_f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gaQr1XsMHEUc"
      },
      "outputs": [],
      "source": [
        "def standardize_text(text):\n",
        "    # Check if text is a string\n",
        "    if isinstance(text, str):\n",
        "        # Convert text to lowercase\n",
        "        return text.lower()\n",
        "    else:\n",
        "        # If text is not a string (i.e., NaN), return an empty string or handle it as desired\n",
        "        return ''\n",
        "\n",
        "# Apply text standardization to 'transcription' column\n",
        "df['transcription'] = df['transcription'].apply(standardize_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropping Duplicates and Checking DataFrame Structure"
      ],
      "metadata": {
        "id": "6M1chno9OmRB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hafo9smCHEUc",
        "outputId": "6779602b-f139-4115-88f0-5df86bb6193f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                        description  \\\n",
            "0           0   A 23-year-old white female presents with comp...   \n",
            "1           1           Consult for laparoscopic gastric bypass.   \n",
            "2           2           Consult for laparoscopic gastric bypass.   \n",
            "3           3                             2-D M-Mode. Doppler.     \n",
            "4           4                                 2-D Echocardiogram   \n",
            "\n",
            "             medical_specialty                                sample_name  \\\n",
            "0         Allergy / Immunology                         Allergic Rhinitis    \n",
            "1                   Bariatrics   Laparoscopic Gastric Bypass Consult - 2    \n",
            "2                   Bariatrics   Laparoscopic Gastric Bypass Consult - 1    \n",
            "3   Cardiovascular / Pulmonary                    2-D Echocardiogram - 1    \n",
            "4   Cardiovascular / Pulmonary                    2-D Echocardiogram - 2    \n",
            "\n",
            "                                       transcription  \\\n",
            "0  subjective:,  this 23-year-old white female pr...   \n",
            "1  past medical history:, he has difficulty climb...   \n",
            "2  history of present illness: , i have seen abc ...   \n",
            "3  2-d m-mode: , ,1.  left atrial enlargement wit...   \n",
            "4  1.  the left ventricular cavity size and wall ...   \n",
            "\n",
            "                                            keywords  \n",
            "0  allergy / immunology, allergic rhinitis, aller...  \n",
            "1  bariatrics, laparoscopic gastric bypass, weigh...  \n",
            "2  bariatrics, laparoscopic gastric bypass, heart...  \n",
            "3  cardiovascular / pulmonary, 2-d m-mode, dopple...  \n",
            "4  cardiovascular / pulmonary, 2-d, doppler, echo...  \n"
          ]
        }
      ],
      "source": [
        "# Drop Duplicates\n",
        "def drop_duplicates(df):\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    return df\n",
        "\n",
        "# Apply dropping duplicates\n",
        "df = drop_duplicates(df)\n",
        "\n",
        "# Check the first few rows of the DataFrame to understand its structure\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Text Cleaning Function\n"
      ],
      "metadata": {
        "id": "4BJY0CQbOtx7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dDc1k1k8HEUc"
      },
      "outputs": [],
      "source": [
        "# Define a function to perform text cleaning operations\n",
        "def clean_text(text):\n",
        "    # Check if text is a string or bytes-like object\n",
        "    if isinstance(text, str):\n",
        "        # Remove missing values entry in starting of NLP cleaning methods\n",
        "        # Check if the text starts with 'missing values'\n",
        "        if text.startswith('missing values'):\n",
        "            text = text[len('missing values'):].strip()\n",
        "\n",
        "        # Tokenization\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "\n",
        "        # Stopword removal\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "        # Lemmatization\n",
        "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "        # Remove non-alphanumeric characters\n",
        "        tokens = [re.sub(r'[^a-zA-Z0-9\\s]', '', token) for token in tokens]\n",
        "\n",
        "        # Remove URLs\n",
        "        tokens = [re.sub(r'http\\S+', '', token) for token in tokens]\n",
        "\n",
        "        # Convert text to lowercase\n",
        "        tokens = [token.lower() for token in tokens]\n",
        "\n",
        "        # Join the tokens back into a single string\n",
        "        cleaned_text = ' '.join(tokens)\n",
        "    else:\n",
        "        # If text is not a string, return empty string\n",
        "        cleaned_text = ''\n",
        "\n",
        "    return cleaned_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying Text Cleaning and Checking DataFrame after Cleaning"
      ],
      "metadata": {
        "id": "QgATJIIWO936"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKh-VUkKHEUc",
        "outputId": "cb31e2d4-ae92-4661-cd91-b02e30157a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                        description  \\\n",
            "0           0   A 23-year-old white female presents with comp...   \n",
            "1           1           Consult for laparoscopic gastric bypass.   \n",
            "2           2           Consult for laparoscopic gastric bypass.   \n",
            "3           3                             2-D M-Mode. Doppler.     \n",
            "4           4                                 2-D Echocardiogram   \n",
            "\n",
            "             medical_specialty                                sample_name  \\\n",
            "0         Allergy / Immunology                         Allergic Rhinitis    \n",
            "1                   Bariatrics   Laparoscopic Gastric Bypass Consult - 2    \n",
            "2                   Bariatrics   Laparoscopic Gastric Bypass Consult - 1    \n",
            "3   Cardiovascular / Pulmonary                    2-D Echocardiogram - 1    \n",
            "4   Cardiovascular / Pulmonary                    2-D Echocardiogram - 2    \n",
            "\n",
            "                                       transcription  \\\n",
            "0  subjective:,  this 23-year-old white female pr...   \n",
            "1  past medical history:, he has difficulty climb...   \n",
            "2  history of present illness: , i have seen abc ...   \n",
            "3  2-d m-mode: , ,1.  left atrial enlargement wit...   \n",
            "4  1.  the left ventricular cavity size and wall ...   \n",
            "\n",
            "                                            keywords  \\\n",
            "0  allergy / immunology, allergic rhinitis, aller...   \n",
            "1  bariatrics, laparoscopic gastric bypass, weigh...   \n",
            "2  bariatrics, laparoscopic gastric bypass, heart...   \n",
            "3  cardiovascular / pulmonary, 2-d m-mode, dopple...   \n",
            "4  cardiovascular / pulmonary, 2-d, doppler, echo...   \n",
            "\n",
            "                               cleaned_transcription  \n",
            "0  subjective   23yearold white female present co...  \n",
            "1  past medical history   difficulty climbing sta...  \n",
            "2  history present illness   seen abc today  plea...  \n",
            "3  2d mmode   1 left atrial enlargement left atri...  \n",
            "4  1 left ventricular cavity size wall thickness ...  \n"
          ]
        }
      ],
      "source": [
        "# Apply the cleaning function to the 'transcription' column\n",
        "df['cleaned_transcription'] = df['transcription'].apply(clean_text)\n",
        "\n",
        "# Check the first few rows of the DataFrame after cleaning\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Cleaned DataFrame\n"
      ],
      "metadata": {
        "id": "19ODoJ6VPVVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the cleaned DataFrame to a new CSV file\n",
        "df.to_csv('cleaned_file.csv', index=False)\n"
      ],
      "metadata": {
        "id": "rpSNPB08PXyZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmX5U3DiHEUk"
      },
      "source": [
        "## Organizing The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zrDV5hqHEUk"
      },
      "source": [
        "### Assignment:\n",
        "1. Organized data in two standard text formats:\n",
        "   a) Corpus - corpus is a collection of texts, and they are all put together neatly in a pandas dataframe here.\n",
        "   b) Document-Term Matrix - word counts in matrix format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSEyHx4SHEUk"
      },
      "source": [
        "### Corpus: Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6ZTO8qPHEUk"
      },
      "source": [
        "A corpus is a collection of texts, and they are all put together neatly in a pandas dataframe here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YkOL7R8KHEUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7038c45d-1710-4b5a-d347-cc6983ff6e19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Unnamed: 0                                        description  \\\n",
            "0              0   A 23-year-old white female presents with comp...   \n",
            "1              1           Consult for laparoscopic gastric bypass.   \n",
            "2              2           Consult for laparoscopic gastric bypass.   \n",
            "3              3                             2-D M-Mode. Doppler.     \n",
            "4              4                                 2-D Echocardiogram   \n",
            "...          ...                                                ...   \n",
            "4984        4984   Patient suffered from morbid obesity for many...   \n",
            "4985        4985   Patient presented to the Bariatric Surgery Se...   \n",
            "4989        4989   Evaluation for elective surgical weight loss ...   \n",
            "4993        4993   Chronic glossitis, xerostomia, probable envir...   \n",
            "4995        4995   This is a 14-month-old baby boy Caucasian who...   \n",
            "\n",
            "                medical_specialty  \\\n",
            "0            Allergy / Immunology   \n",
            "1                      Bariatrics   \n",
            "2                      Bariatrics   \n",
            "3      Cardiovascular / Pulmonary   \n",
            "4      Cardiovascular / Pulmonary   \n",
            "...                           ...   \n",
            "4984                   Bariatrics   \n",
            "4985                   Bariatrics   \n",
            "4989                   Bariatrics   \n",
            "4993         Allergy / Immunology   \n",
            "4995         Allergy / Immunology   \n",
            "\n",
            "                                         sample_name  \\\n",
            "0                                 Allergic Rhinitis    \n",
            "1           Laparoscopic Gastric Bypass Consult - 2    \n",
            "2           Laparoscopic Gastric Bypass Consult - 1    \n",
            "3                            2-D Echocardiogram - 1    \n",
            "4                            2-D Echocardiogram - 2    \n",
            "...                                              ...   \n",
            "4984             Discharge Summary - Gastric Bypass    \n",
            "4985   Bariatric Consult - Surgical Weight Loss - 4    \n",
            "4989   Bariatric Consult - Surgical Weight Loss - 2    \n",
            "4993                        Evaluation of Allergies    \n",
            "4995           Kawasaki Disease - Discharge Summary    \n",
            "\n",
            "                                          transcription  \\\n",
            "0     subjective:,  this 23-year-old white female pr...   \n",
            "1     past medical history:, he has difficulty climb...   \n",
            "2     history of present illness: , i have seen abc ...   \n",
            "3     2-d m-mode: , ,1.  left atrial enlargement wit...   \n",
            "4     1.  the left ventricular cavity size and wall ...   \n",
            "...                                                 ...   \n",
            "4984  admission diagnosis:,  morbid obesity.  bmi is...   \n",
            "4985  history of present illness:,  ms. a is a 55-ye...   \n",
            "4989  past medical history:  ,she had a negative str...   \n",
            "4993  history:,  a 55-year-old female presents self-...   \n",
            "4995  admitting diagnosis: , kawasaki disease.,disch...   \n",
            "\n",
            "                                               keywords  \\\n",
            "0     allergy / immunology, allergic rhinitis, aller...   \n",
            "1     bariatrics, laparoscopic gastric bypass, weigh...   \n",
            "2     bariatrics, laparoscopic gastric bypass, heart...   \n",
            "3     cardiovascular / pulmonary, 2-d m-mode, dopple...   \n",
            "4     cardiovascular / pulmonary, 2-d, doppler, echo...   \n",
            "...                                                 ...   \n",
            "4984  bariatrics, laparoscopic gastric bypass, gastr...   \n",
            "4985  bariatrics, jenny craig, medifast, nutrisystem...   \n",
            "4989  bariatrics, elective surgical weight loss, sur...   \n",
            "4993  allergy / immunology, chronic glossitis, xeros...   \n",
            "4995  allergy / immunology, mucous membranes, conjun...   \n",
            "\n",
            "                                  cleaned_transcription  \n",
            "0     subjective   23yearold white female present co...  \n",
            "1     past medical history   difficulty climbing sta...  \n",
            "2     history present illness   seen abc today  plea...  \n",
            "3     2d mmode   1 left atrial enlargement left atri...  \n",
            "4     1 left ventricular cavity size wall thickness ...  \n",
            "...                                                 ...  \n",
            "4984  admission diagnosis   morbid obesity  bmi 51  ...  \n",
            "4985  history present illness   ms 55yearold female ...  \n",
            "4989  past medical history   negative stress test fo...  \n",
            "4993  history   55yearold female present selfreferre...  \n",
            "4995  admitting diagnosis   kawasaki disease  discha...  \n",
            "\n",
            "[3813 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "# Let's take a look at our dataframe\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Unnamed: 0': [0, 1, 2],\n",
        "    'description': [\"A 23-year-old white female presents with comp...\",\n",
        "                    \"Consult for laparoscopic gastric bypass.\",\n",
        "                    \"2-D M-Mode. Doppler.\"],\n",
        "    'medical_specialty': [\"Allergy / Immunology\", \"Bariatrics\", \"Cardiovascular / Pulmonary\"],\n",
        "    'sample_name': [\"Allergic Rhinitis\", \"Laparoscopic Gastric Bypass Consult - 2\", \"2-D Echocardiogram - 1\"],\n",
        "    'transcription': [\"subjective: This 23-year-old white female pr...\",\n",
        "                      \"past medical history: he has difficulty climb...\",\n",
        "                      \"2-d m-mode: , ,1. left atrial enlargement wit...\"],\n",
        "    'keywords': [\"allergy / immunology, allergic rhinitis, aller...\",\n",
        "                 \"bariatrics, laparoscopic gastric bypass, weigh...\",\n",
        "                 \"cardiovascular / pulmonary, 2-d m-mode, dopple...\"],\n",
        "    'cleaned_transcription': [\"subjective 23yearold white female present co...\",\n",
        "                              \"past medical history difficulty climbing sta...\",\n",
        "                              \"2d mmode 1 left atrial enlargement left atri...\"]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve7lGZ--wQs6",
        "outputId": "34c9e14d-fae5-4cdc-e297-481d9cfc572f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                       description  \\\n",
            "0           0  A 23-year-old white female presents with comp...   \n",
            "1           1          Consult for laparoscopic gastric bypass.   \n",
            "2           2                              2-D M-Mode. Doppler.   \n",
            "\n",
            "            medical_specialty                              sample_name  \\\n",
            "0        Allergy / Immunology                        Allergic Rhinitis   \n",
            "1                  Bariatrics  Laparoscopic Gastric Bypass Consult - 2   \n",
            "2  Cardiovascular / Pulmonary                   2-D Echocardiogram - 1   \n",
            "\n",
            "                                      transcription  \\\n",
            "0   subjective: This 23-year-old white female pr...   \n",
            "1  past medical history: he has difficulty climb...   \n",
            "2  2-d m-mode: , ,1. left atrial enlargement wit...   \n",
            "\n",
            "                                            keywords  \\\n",
            "0  allergy / immunology, allergic rhinitis, aller...   \n",
            "1  bariatrics, laparoscopic gastric bypass, weigh...   \n",
            "2  cardiovascular / pulmonary, 2-d m-mode, dopple...   \n",
            "\n",
            "                             cleaned_transcription  \n",
            "0  subjective 23yearold white female present co...  \n",
            "1  past medical history difficulty climbing sta...  \n",
            "2  2d mmode 1 left atrial enlargement left atri...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm0hzgLIHEUl"
      },
      "source": [
        "### Document-Term Matrix: Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynb4EGQRHEUl"
      },
      "source": [
        "For many of the techniques we'll be using in future assignment, the text must be tokenized, meaning broken down into smaller pieces. The most common tokenization technique is to break down text into words. We can do this using scikit-learn's ` CountVectorizer `, where every row will represent a different document and every column will represent a different word.\n",
        "\n",
        "In addition, with ` CountVectorizer `, we can remove stop words. Stop words are common words that add no additional meaning to text such as 'a', 'the', etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "u8afsU65HEUl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f66a9a32-1939-4026-b16e-1f5bbb17182a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      00  000  0000000  000units  001  004  00am  00pm  01  013  ...  zuba  \\\n",
            "0      0    0        0         0    0    0     0     0   0    0  ...     0   \n",
            "1      0    0        0         0    0    0     0     0   0    0  ...     0   \n",
            "2      0    0        0         0    0    0     0     0   0    0  ...     0   \n",
            "3      0    0        0         0    0    0     0     0   0    0  ...     0   \n",
            "4      0    0        0         0    0    0     0     0   0    0  ...     0   \n",
            "...   ..  ...      ...       ...  ...  ...   ...   ...  ..  ...  ...   ...   \n",
            "4984   0    0        0         0    0    0     0     0   0    0  ...     0   \n",
            "4985   0    1        0         0    0    0     0     0   0    0  ...     0   \n",
            "4989   0    0        0         0    0    0     0     0   0    0  ...     0   \n",
            "4993   0    0        0         0    0    0     0     0   0    0  ...     0   \n",
            "4995   0    0        0         0    0    0     0     0   0    0  ...     0   \n",
            "\n",
            "      zumi  zung  zygoma  zygomatic  zymar  zyprexa  zyrtec  zyvox  µiu  \n",
            "0        0     0       0          0      0        0       2      0    0  \n",
            "1        0     0       0          0      0        0       0      0    0  \n",
            "2        0     0       0          0      0        0       0      0    0  \n",
            "3        0     0       0          0      0        0       0      0    0  \n",
            "4        0     0       0          0      0        0       0      0    0  \n",
            "...    ...   ...     ...        ...    ...      ...     ...    ...  ...  \n",
            "4984     0     0       0          0      0        0       0      0    0  \n",
            "4985     0     0       0          0      0        0       0      0    0  \n",
            "4989     0     0       0          0      0        0       0      0    0  \n",
            "4993     0     0       0          0      0        0       0      0    0  \n",
            "4995     0     0       0          0      0        0       0      0    0  \n",
            "\n",
            "[3813 rows x 18288 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Assuming your DataFrame is named df\n",
        "# Replace 'transcription' with the actual column containing your text data\n",
        "# Make sure to preprocess your text data if necessary\n",
        "\n",
        "# Create CountVectorizer with English stop words excluded\n",
        "cv = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Fit and transform the text data\n",
        "data_cv = cv.fit_transform(df['transcription'])\n",
        "\n",
        "# Convert the document-term matrix to a DataFrame\n",
        "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())\n",
        "\n",
        "# Set the index of the DataFrame\n",
        "data_dtm.index = df.index\n",
        "\n",
        "# Display the document-term matrix\n",
        "print(data_dtm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Ry8BRgr9HEUl"
      },
      "source": [
        "## Additional Assignments:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r7N4fG6HEUl"
      },
      "source": [
        "1. Can you add an additional regular expression to the clean_text_round2 function to further clean the text?\n",
        "2. Play around with CountVectorizer's parameters. What is ngram_range? What is min_df and max_df?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "collapsed": true,
        "id": "peUr3o11HEUl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "790dde2d-b616-4c0c-d607-8ce05e5e7619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The price is  and its available at \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def clean_text_round2(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Remove punctuation (including apostrophes)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Additional regular expression to remove specific pattern\n",
        "    text = re.sub(r'\\b\\d+\\b', '', text)  # Remove any standalone numbers\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example usage:\n",
        "cleaned_text = clean_text_round2(\"The price is $10.99, and it's available at https://example.com\")\n",
        "print(cleaned_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer Parameters\n",
        "\n",
        "**1. ngram_range:**\n",
        "ngram_range specifies the range of n-values for different n-grams to be extracted.\n",
        "An n-gram is a contiguous sequence of n items (words, characters, etc.) from a given text.\n",
        "It takes a tuple (min_n, max_n) where min_n is the minimum value of n and max_n is the maximum value of n.\n",
        "For example, ngram_range=(1, 2) indicates that both unigrams (single words) and bigrams (pairs of consecutive words) will be considered.\n",
        "\n",
        "**2. min_df:**\n",
        "min_df specifies the minimum document frequency for a term to be included in the vocabulary.\n",
        "A term must appear in at least min_df documents to be considered.\n",
        "It can be specified as an integer (e.g., min_df=2, meaning the term must appear in at least 2 documents) or as a float between 0 and 1 (e.g., min_df=0.1, meaning the term must appear in at least 10% of the documents).\n",
        "\n",
        "**3. max_df:**\n",
        "max_df specifies the maximum document frequency for a term to be included in the vocabulary.\n",
        "A term must appear in at most max_df documents to be considered.\n",
        "It can be specified as an integer (e.g., max_df=100, meaning the term must appear in at most 100 documents) or as a float between 0 and 1 (e.g., max_df=0.9, meaning the term must appear in at most 90% of the documents).\n",
        "By adjusting these parameters, you can control the size and content of the vocabulary created by CountVectorizer, which can have an impact on the performance of text classification or other natural language processing tasks."
      ],
      "metadata": {
        "id": "w59lUFpMuql2"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
